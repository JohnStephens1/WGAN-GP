{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6d8f80-c4fa-4fa0-b68d-c4f03127c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ca8cd6-8384-4eee-bd40-75923673bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path.cwd()\n",
    "GENERATED_IMAGES_DIR = CWD/'generated_images'\n",
    "CHECKPOINT_DIR = GENERATED_IMAGES_DIR/'checkpoints'\n",
    "SINGLE_IMAGE_DIR = GENERATED_IMAGES_DIR/'single'\n",
    "COLLAGE_DIR = GENERATED_IMAGES_DIR/'collage'\n",
    "DATASET_PATH = Path(r'E:\\datasets\\gan-getting-started')\n",
    "FOLDERS_TO_SETUP = [\n",
    "\tCHECKPOINT_DIR,\n",
    "\tGENERATED_IMAGES_DIR,\n",
    "\tSINGLE_IMAGE_DIR,\n",
    "\tCOLLAGE_DIR\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadcfcaf-bccb-4f7c-8541-bbc69fc1bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d5c2fe-e8a9-49d1-a2b7-d6422efff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "\tpath.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4b6562-7120-42e1-b78d-904388c7c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_folders(paths: list):\n",
    "\tfor path in paths:\n",
    "\t\tmkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15e7b36b-5f8a-47a9-a6f8-8cc56c08a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "\tplt.imshow(image.astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20098b5-fb04-495b-96f9-852f6675338b",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a653bd1-9d61-4386-9d88-bf6e09936067",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (64, 64, 3)\n",
    "BATCHSIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe17b39-2f88-4746-b07b-e94ad599a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(images):\n",
    "\treturn (images-127.5)/127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8731882-831b-453b-aa1d-e7519e07af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_images(images):\n",
    "\treturn np.array(((images*127.5)+127.5), dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2e463293-f0b9-403b-8893-f69efbf1fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images, new_size=IMAGE_SHAPE[:-1], interpolation=cv2.INTER_AREA):\n",
    "\treturn np.array([cv2.resize(image, new_size, interpolation=interpolation) for image in images], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b5b32ab-6920-4887-84c1-33ea5ad88dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images():\n",
    "\timages = np.array([plt.imread(image) for image in (DATASET_PATH/'monet_jpg').glob('*')], dtype='float32')\n",
    "\tassert images.shape == (images.shape[0], 256, 256, 3), 'input images have the wrong dimensions!'\n",
    "\tresized_images = resize_images(images)\n",
    "\tnormalized_images = normalize_images(resized_images)\n",
    "\treturn normalized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaac5cd-5c4b-4352-8cb4-476e0f3c0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = prepare_images()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(images.shape[0]).batch(BATCHSIZE).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ed8b7-7ecc-47d6-8a0d-cdbbb9110791",
   "metadata": {},
   "source": [
    "#### model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55318909-2cbd-4c22-9261-6b962e12f98c",
   "metadata": {},
   "source": [
    "##### blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e7385ec-bdeb-4320-997e-fede2f77f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e38824-ff40-41c3-bd31-ba1202793044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_block(x, n_filters, kernel_size=(3, 3), strides=(2, 2), padding='same'):\n",
    "\tx = layers.Conv2D(n_filters, kernel_size, strides, padding)(x)\n",
    "\tx = layers.LeakyReLU(0.2)(x)\n",
    "\tx = layers.Dropout(0.3)(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7394e97-b60b-462b-93a6-887d17234602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_conv_block(x, n_filters, kernel_size=(3, 3), strides=(2, 2), padding='same'):\n",
    "\tx = layers.Conv2D(n_filters, kernel_size, strides, padding)(x)\n",
    "\tx = layers.LeakyReLU(0.2)(x)\n",
    "\tx = layers.LayerNormalization()(x)\n",
    "\tx = layers.Dropout(0.3)(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aab51c21-dc37-40a2-824e-2ec8a51d2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposed_conv_block(x, n_filters, kernel_size=(3, 3), strides=(2, 2), padding=('same')):\n",
    "\tx = layers.Conv2DTranspose(n_filters, kernel_size, strides, padding, use_bias=False)(x)\n",
    "\tx = layers.LeakyReLU(0.2)(x)\n",
    "\tx = layers.BatchNormalization()(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b4c6b37-ba2f-4f8d-822c-42a281ae7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiplied_image_dimension(current_dimensions, factor):\n",
    "\treturn [dimension*factor for dimension in current_dimensions[-2:-4:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84505ed8-03a6-49d1-9369-678f89d14df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizing_conv_block(x, n_filters, kernel_size=(3, 3), strides=(1, 1), padding='same'):\n",
    "\tx = layers.Resizing(*get_multiplied_image_dimension(x.shape, 0.5), interpolation='nearest')(x)\n",
    "\tx = layers.Conv2D(n_filters, kernel_size, strides, padding)\n",
    "\tx = layers.LeakyReLU(0.2)(x)\n",
    "\tx = layers.LayerNormalization()(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ff225d-efc4-46bf-aec3-6af89c9c7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizing_transposed_conv_block(x, n_filters, kernel_size=(3, 3), strides=(1, 1), padding='same'):\n",
    "\tx = layers.Resizing(*get_multiplied_image_dimension(x.shape, 2), interpolation='nearest')(x)\n",
    "\tx = layers.Conv2DTranspose(n_filters, kernel_size, strides, padding)(x)\n",
    "\tx = layers.LeakyReLU(0.2)(x)\n",
    "\tx = layers.BatchNormalization()(x)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081f449-2b36-4def-a0d7-889d426cf1f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22511423-8e5c-4e60-ab37-134506109560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "\ttf.keras.backend.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69921561-54b4-43ff-ae4f-b0d21250a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "\tdef __init__(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.optimizer = tf.keras.optimizers.Adam(1e-4, 0, 0.9)\n",
    "\t\tself.model = self.build()\n",
    "\t\t\n",
    "\tdef __call__(self, inputs, training=False):\n",
    "\t\treturn self.model(inputs, training)\n",
    "\t\n",
    "\tdef build(self):\n",
    "\t\tinitial_dimensions = (8, 8)\n",
    "\t\tinputs = layers.Input(self.input_size)\n",
    "\n",
    "\t\tx = layers.Dense(np.product(initial_dimensions)*256, use_bias=False)(inputs)\n",
    "\t\tx = layers.BatchNormalization()(x)\n",
    "\t\tx = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "\t\tx = layers.Reshape((*initial_dimensions, 256))(x)\n",
    "\t\t\n",
    "\t\tx = resizing_transposed_conv_block(x, 256)\n",
    "\t\tx = resizing_transposed_conv_block(x, 128)\n",
    "\t\tx = resizing_transposed_conv_block(x, 64)\n",
    "\t\tx = resizing_transposed_conv_block(x, 32)\n",
    "\n",
    "\t\toutputs = resizing_transposed_conv_block(x, 3)\n",
    "\t\t# assert outputs.shape == [None, 256, 256, 3], f'output tensor\\'s shapes are wrong, {outputs.shape}'\n",
    "\n",
    "\t\tmodel = tf.keras.Model(inputs, outputs, name='generator')\n",
    "\n",
    "\t\treturn model\n",
    "\t\n",
    "\tdef compute_loss(self, fake_image):\n",
    "\t\treturn -tf.reduce_mean(fake_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7184f975-2520-4b61-b51f-9aa8c502346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "\tdef __init__(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.optimizer = tf.keras.optimizers.Adam(1e-4, 0, 0.9)\n",
    "\t\tself.model = self.build()\n",
    "\t\t\n",
    "\tdef __call__(self, inputs, training=False):\n",
    "\t\treturn self.model(inputs, training=training)\n",
    "\t\n",
    "\tdef build(self):\n",
    "\t\tinputs = layers.Input(self.input_size)\n",
    "\n",
    "\t\tx = normalized_conv_block(inputs, 64, (5, 5))\n",
    "\t\tx = normalized_conv_block(x, 128)\n",
    "\t\tx = normalized_conv_block(x, 128)\n",
    "\t\tx = normalized_conv_block(x, 128)\n",
    "\n",
    "\t\tx = layers.Flatten()(x)\n",
    "\t\tx = layers.Dropout(0.2)(x)\n",
    "\t\toutputs = layers.Dense(1)(x)\n",
    "\n",
    "\t\tmodel = tf.keras.Model(inputs, outputs, name='critic')\n",
    "\n",
    "\t\treturn model\n",
    "\t\n",
    "\tdef compute_loss(self, real_predictions, fake_predictions):\n",
    "\t\treal_loss = tf.reduce_mean(real_predictions)\n",
    "\t\tfake_loss = tf.reduce_mean(fake_predictions)\n",
    "\t\treturn fake_loss - real_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b924d5fc-a528-428e-b758-d2800a89b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(tf.keras.Model):\n",
    "\tdef __init__(self, generator, critic, latent_dim, batchsize=BATCHSIZE, critic_extra_steps=5, alpha=1e-4, beta1=0, beta2=0.9, lambd=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.generator = generator\n",
    "\t\tself.critic = critic\n",
    "\t\tself.optimizer = tf.keras.optimizers.Adam(alpha, beta1, beta2)\n",
    "\t\tself.latent_dim = latent_dim\n",
    "\t\tself.critic_extra_steps = critic_extra_steps\n",
    "\t\tself.batchsize = BATCHSIZE\n",
    "\t\tself.lambd = lambd\n",
    "\t\tself.last_epoch = 0\n",
    "\n",
    "\tdef compute_gradient_penalty(self, generated_images, real_images):\n",
    "\t\tepsilon = tf.random.uniform((tf.shape(real_images)[0], 1, 1, 1), 0, 1)\n",
    "\t\tinterpolated_generated_images = epsilon * real_images + (1-epsilon) * generated_images\n",
    "\n",
    "\t\twith tf.GradientTape() as gp_tape:\n",
    "\t\t\tgp_tape.watch(interpolated_generated_images)\n",
    "\t\t\tprediction = critic(interpolated_generated_images, training=True)\n",
    "\n",
    "\t\tgrads = gp_tape.gradient(prediction, interpolated_generated_images)\n",
    "\t\t\n",
    "\t\t# norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=(1, 2, 3)))\n",
    "\n",
    "\t\t# in the official paper the formula suggested to be only used on singular examples, therefore omit batchnormalization. This might be an alternative\n",
    "\t\tnorm = tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(grads), axis=(1, 2, 3))))\n",
    "\t\tgradient_penalty = self.lambd * tf.square(norm - 1)\n",
    "\n",
    "\t\treturn gradient_penalty\n",
    "\n",
    "\tdef train_step(self, images):\n",
    "\t\tfor _ in range(self.critic_extra_steps):\n",
    "\t\t\tnoise = tf.random.normal(shape=(tf.shape(images)[0], self.latent_dim))\n",
    "\t\t\t\n",
    "\t\t\twith tf.GradientTape() as tape:\n",
    "\t\t\t\tgenerated_images = self.generator(noise, training=True)\n",
    "\t\t\t\tfake_predictions = self.critic(generated_images, training=True)\n",
    "\t\t\t\treal_predictions = self.critic(images, training=True)\n",
    "\t\t\t\n",
    "\t\t\t\tc_cost = self.critic.compute_loss(real_predictions, fake_predictions)\n",
    "\t\t\t\tgradient_penalty = self.compute_gradient_penalty(generated_images, images)\n",
    "\t\t\t\tc_loss = c_cost + gradient_penalty\n",
    "\t\t\n",
    "\t\t\tc_gradient = tape.gradient(c_loss, self.critic.model.trainable_variables)\n",
    "\t\t\tself.critic.optimizer.apply_gradients(zip(c_gradient, self.critic.model.trainable_variables))\n",
    "\n",
    "\t\tnoise = tf.random.normal((self.batchsize, self.latent_dim))\n",
    "\t\t\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\tgenerated_images = self.generator(noise, training=True)\n",
    "\t\t\tfake_predictions = self.critic(generated_images, training=True)\n",
    "\t\t\tg_loss = self.generator.compute_loss(fake_predictions)\n",
    "\t\t\n",
    "\t\tg_gradient = tape.gradient(g_loss, self.generator.model.trainable_variables)\n",
    "\t\tself.generator.optimizer.apply_gradients(zip(g_gradient, self.generator.model.trainable_variables))\n",
    "\t\t\n",
    "\t\treturn {'c_loss': c_loss, 'g_loss': g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09cdeb5b-7cef-409d-b959-c71cd5bde66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "\tdef __init__(self, latent_dim=100):\n",
    "\t\tself.latent_dim = latent_dim\n",
    "\t\t\n",
    "\tdef on_epoch_end(self, epoch, logs=None):\n",
    "\t\tnoise = tf.random.normal((12, self.latent_dim))\n",
    "\t\tgenerated_images = self.model.generator(noise)\n",
    "\t\tgenerated_images = denormalize_images(generated_images)\n",
    "\t\n",
    "\t\tdisplay.clear_output(True)\n",
    "\t\t\n",
    "\t\tf, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\t\tfor i, ax in enumerate(axs.flatten()):\n",
    "\t\t\tax.imshow(generated_images[i])\n",
    "\t\t\tax.axis('off')\n",
    "\t\tplt.show()\n",
    "\t\t\n",
    "\t\tif epoch % 25 == 0:\n",
    "\t\t\tcheckpoint.save(CHECKPOINT_DIR/'checkpoint')\n",
    "\t\t\t\n",
    "\t\tif epoch % 20 == 0:\n",
    "\t\t\tf.savefig(COLLAGE_DIR/f'image_at_epoch_{epoch}.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f20e4-8e0a-4796-b029-f608954a7a40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94c12402-72fd-44f9-b8ab-5167abf8e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4845f4b-8dcd-4f2e-ac5a-fa9a2cd381ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_monitor = GANMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd795b4f-c7d8-4383-b6c1-9824e09941ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(LATENT_DIM)\n",
    "critic = Critic(IMAGE_SHAPE)\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "\tgenerator_optimizer=generator.optimizer,\n",
    "\tcritic_optimizer=critic.optimizer,\n",
    "\tgenerator=generator.model,\n",
    "\tcritic=critic.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fcbb10d-810b-4b08-a612-67e062e835a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan = WGAN(generator, critic, LATENT_DIM)\n",
    "wgan.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "173fbf4d-4915-4035-a8cb-e8ecef87b3a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'critic/layer_normalization_4/mul_9' defined at (most recent call last):\n    File \"D:\\Languages\\python\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Languages\\python\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Languages\\python\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"D:\\Languages\\python\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"D:\\Languages\\python\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_11736\\4246447266.py\", line 3, in <cell line: 3>\n      wgan.fit(train_dataset, shuffle=True, epochs=EPOCHS, callbacks=[gan_monitor], verbose=0)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_11736\\3909380708.py\", line 38, in train_step\n      real_predictions = self.critic(images, training=True)\n    File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_11736\\1732911612.py\", line 8, in __call__\n      return self.model(inputs, training=training)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\layers\\normalization\\layer_normalization.py\", line 322, in call\n      outputs = outputs * tf.cast(scale, outputs.dtype)\nNode: 'critic/layer_normalization_4/mul_9'\nfailed to allocate memory\n\t [[{{node critic/layer_normalization_4/mul_9}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_37935]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      2\u001b[0m setup_folders(FOLDERS_TO_SETUP)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mwgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mgan_monitor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'critic/layer_normalization_4/mul_9' defined at (most recent call last):\n    File \"D:\\Languages\\python\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Languages\\python\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Languages\\python\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"D:\\Languages\\python\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"D:\\Languages\\python\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_11736\\4246447266.py\", line 3, in <cell line: 3>\n      wgan.fit(train_dataset, shuffle=True, epochs=EPOCHS, callbacks=[gan_monitor], verbose=0)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_11736\\3909380708.py\", line 38, in train_step\n      real_predictions = self.critic(images, training=True)\n    File \"C:\\Users\\johna\\AppData\\Local\\Temp\\ipykernel_11736\\1732911612.py\", line 8, in __call__\n      return self.model(inputs, training=training)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\johna\\Documents\\A_Documents\\Programming\\Python\\.my_envs\\datascience\\lib\\site-packages\\keras\\layers\\normalization\\layer_normalization.py\", line 322, in call\n      outputs = outputs * tf.cast(scale, outputs.dtype)\nNode: 'critic/layer_normalization_4/mul_9'\nfailed to allocate memory\n\t [[{{node critic/layer_normalization_4/mul_9}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_37935]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "setup_folders(FOLDERS_TO_SETUP)\n",
    "wgan.fit(train_dataset, shuffle=True, epochs=EPOCHS, callbacks=[gan_monitor], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2fa3b-f65c-446e-8efe-aef71d9905c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models():\n",
    "\twgan.generator.model.save(CWD/'generator')\n",
    "\twgan.critic.model.save(CWD/'critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89675ca7-29f1-408a-a326-d40de3301418",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wgan.history.history['c_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc412c19-de32-4ecf-82bc-06792acaafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal((16, 100))\n",
    "generated_images = wgan.generator(noise)\n",
    "generated_images = denormalize_images(generated_images)\n",
    "\n",
    "display.clear_output(True)\n",
    "\n",
    "for i in range(16):\n",
    "\tplt.subplot(4, 4, i+1)\n",
    "\tplt.imshow(generated_images[i])\n",
    "\tplt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4fb548-0764-425e-8cae-2125fb65b447",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- tune hyperparameters\n",
    "- maybe add loss to make output pictures more vibrant\n",
    "- check for reasons for 'noise explosions' in output\n",
    "\n",
    "\n",
    "- prepare model for being trained on cloud, kaggle and / or google colab\n",
    "- see how to increase hardware usage, multi core processing and on cloud tpu usage\n",
    "\t-> auto tune shenanigans, distribution strategy and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34b70b-b80b-451f-b081-9c1f95cd2e56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### refrences\n",
    "http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson5/WGAN-GP.pdf  \n",
    "https://keras.io/examples/generative/wgan_gp/  \n",
    "https://github.com/caogang/wgan-gp/blob/master/gan_mnist.py  \n",
    "https://developers.google.com/machine-learning/gan/loss  \n",
    "https://www.youtube.com/watch?v=pG0QZ7OddX4  \n",
    "https://distill.pub/2016/deconv-checkerboard/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
